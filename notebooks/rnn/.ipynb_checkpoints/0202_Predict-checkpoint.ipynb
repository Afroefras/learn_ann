{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7195211",
   "metadata": {},
   "source": [
    "# Play with the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ba9f678",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T17:32:10.564118Z",
     "start_time": "2021-04-19T17:32:10.558593Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['glove.6B.300d.txt', '.DS_Store', 'glove.6B.100d.txt', 'corpus.txt', 'glove.6B.50d.txt', 'rnn_asimov_weights.h5', 'project', 'glove.6B.200d.txt', 'rnn_asimov_architecture.json', 'glove.6B.zip', 'rnn_asimov_tokenizer.pickle', 'The_Last_Question.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "BASE_DIR = '/Users/efraflores/Desktop/EF/Diplo/Asimov' #'/content' if you're using GColab\n",
    "print(os.listdir(BASE_DIR)) #Don't you forget to upload tokenizer,architecture and weights if you're using GColab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c41b9b5",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4a13697",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T17:32:15.062666Z",
     "start_time": "2021-04-19T17:32:10.567377Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sleep time: This is how typing a story should\n",
      "looks like! Right?\n",
      "\n",
      "Now without it:\n",
      "This is just a very basic example of how this\n",
      "function split the text structure in order to have a\n",
      "fixed number of words per line instead of just one\n",
      "line showing the whole text.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def print_lines(text,word_p_line=10,sleep=False,sleep_time=0.06):\n",
    "    #Print batches of N words\n",
    "    lines = ''\n",
    "    n = 0\n",
    "    for i,word in enumerate(text.split()):\n",
    "        lines += '\\n' if i//word_p_line==0 else ''\n",
    "        lines += ' '.join(text.split()[n:n+word_p_line])\n",
    "        n += word_p_line\n",
    "    if sleep:\n",
    "        #Show letter by letter to perform as typing\n",
    "        for letter,_ in enumerate(lines.strip()):\n",
    "            clear_output()\n",
    "            print(lines.strip()[:letter+1])\n",
    "            time.sleep(sleep_time)\n",
    "    else:\n",
    "        print(lines.strip())\n",
    "\n",
    "print_lines('With sleep time: This is how typing a story should looks like! Right?',sleep=True)\n",
    "print('\\nNow without it:')\n",
    "print_lines('This is just a very basic example of how this function split the text structure in order to have a fixed number of words per line instead of just one line showing the whole text.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cdab6a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T17:32:18.586259Z",
     "start_time": "2021-04-19T17:32:15.069040Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def Free_AI_Writing(text_model,fitted_tokenizer,next_words=70):\n",
    "    #Reverse the classes in order to call like this: dict[position] --> word\n",
    "    dict_classes = {y:x for x,y in fitted_tokenizer.word_index.items()}\n",
    "    #First string input\n",
    "    text = str(input(f'Give me the first word or sentence:\\n'))\n",
    "    while len(text.split()) < 1:\n",
    "        #It does need at least one word at the beginning!\n",
    "        text = str(input(f'There were no words. Try again\\n'))\n",
    "    else: clear_output()\n",
    "    whole_text = text\n",
    "    for _ in range(int(next_words)):\n",
    "        #Transform the words into the sequence the model needs\n",
    "        seq = fitted_tokenizer.texts_to_sequences([whole_text])[0]\n",
    "        seq = pad_sequences([seq],text_model.layers[0].output_shape[1])\n",
    "        #Predicts an array, brings the most likely class position\n",
    "        word = np.argmax(text_model.predict(seq),axis=-1)\n",
    "        #Concat the word with the previous string\n",
    "        whole_text += ' '+dict_classes[word[0]]\n",
    "    #Print it as typing\n",
    "    print_lines(whole_text,sleep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4ce1306",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T17:32:18.597704Z",
     "start_time": "2021-04-19T17:32:18.589612Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def Input_AI_Writing(text_model,fitted_tokenizer,next_words=10):\n",
    "    #Reverse the classes in order to call like this: dict[position] --> word\n",
    "    dict_classes = {y:x for x,y in fitted_tokenizer.word_index.items()}\n",
    "    #First string input\n",
    "    text = str(input(f'Give me the first word or sentence:\\n'))\n",
    "    while len(text.split()) < 1:\n",
    "        #It does need at least one word!\n",
    "        text = str(input(f\"There were no words. Try again\\n\"))\n",
    "    else: clear_output()\n",
    "    whole_text = text\n",
    "    write = True\n",
    "    #Until user stop playing with the model predictions\n",
    "    while write:\n",
    "        for _ in range(int(next_words)):\n",
    "            #Transform the words into the sequence the model needs\n",
    "            seq = fitted_tokenizer.texts_to_sequences([whole_text])[0]\n",
    "            seq = pad_sequences([seq],text_model.layers[0].output_shape[1])\n",
    "            #Predicts an array, brings the most likely class position\n",
    "            word = np.argmax(model.predict(seq),axis=-1)\n",
    "            #Concat the word with the previous string\n",
    "            whole_text += ' '+dict_classes[word[0]]\n",
    "        #Show the resulting text so far\n",
    "        print_lines(whole_text)\n",
    "        #Ask user to continue generating text given the updated text\n",
    "        stop = input('\\nContinue generating text? y/n\\n')\n",
    "        while stop not in ('y','n'):\n",
    "            #It just accepts the correct input\n",
    "            stop = input('Choose \"y\" to continue or \"n\" to exit\\n')\n",
    "        if stop=='n':\n",
    "            clear_output()\n",
    "            #Show the final result\n",
    "            print_lines(whole_text)\n",
    "            write = False\n",
    "        else:\n",
    "            #Ask for the number of the next predicted-words\n",
    "            next_words = input('How many words more do you want to generate? ')\n",
    "            need_int = True\n",
    "            while need_int:\n",
    "                try:\n",
    "                    int(next_words)\n",
    "                except:\n",
    "                    #It just accepts an integer input\n",
    "                    next_words = input(f'\"{next_words}\" is not an integer, try again! ')\n",
    "                    pass\n",
    "                else: need_int = False\n",
    "            else:\n",
    "                #Ask for another word, sentence or even nothing to continue generating text\n",
    "                whole_text += ' '+str(input(f'Give me another word/sentence or just press \"Enter\" to continue:\\n'))\n",
    "                clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3499e4a4",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb79cacb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T17:32:19.733262Z",
     "start_time": "2021-04-19T17:32:18.600077Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from keras.models import model_from_json\n",
    "\n",
    "with open(os.path.join(BASE_DIR,'rnn_asimov_tokenizer.pickle'), 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(BASE_DIR,'rnn_asimov_architecture.json'), 'r') as f:\n",
    "    model = model_from_json(f.read())\n",
    "\n",
    "#Now, we can use this model whenever we want!\n",
    "model.load_weights(os.path.join(BASE_DIR,'rnn_asimov_weights.h5'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a1569c",
   "metadata": {},
   "source": [
    "## Generate text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1865af0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T17:33:32.877386Z",
     "start_time": "2021-04-19T17:32:41.191075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Man's last mind paused before fusion looking over a space\n",
      "that included nothing but the remaining pieces dregs of one\n",
      "last dark star and nothing besides\n"
     ]
    }
   ],
   "source": [
    "#Play with the model writing together <3\n",
    "Input_AI_Writing(model,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9099547d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T17:27:50.307343Z",
     "start_time": "2021-04-19T17:27:12.329497Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing is real here this chaos not be reversed into\n",
      "the universe once more power units by man had its\n",
      "lives and were self conscious over the momentary sensation of\n",
      "insideoutness drifted freely through sp ace that was called one\n",
      "star for us said man 'when da ta will be\n",
      "sufficient or is the problem insoluble in all concei vable\n",
      "circumstances was somehow not a first move ancient a spaceship\n",
      "to a five dimensional\n"
     ]
    }
   ],
   "source": [
    "#Or let the artificial creativity shows off!\n",
    "Free_AI_Writing(model,tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
